{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebvqJaNU9bkH"
      },
      "source": [
        "# Wprowadzenie do sieci neuronowych i uczenia maszynowego\n",
        "\n",
        "## Lab 4b: Wprowadzenie do biblioteki PyTorch, uczenie sieci neuronowych\n",
        "\n",
        "---\n",
        "\n",
        "**Prowadzący:** Iwo Błądek, Anna Labijak-Kowalska<br>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wlq47LA0BuBB"
      },
      "source": [
        "## Cel ćwiczeń\n",
        "\n",
        "* wprowadzenie biblioteki PyTorch,\n",
        "* ukazanie różnic i podobieństw pomiędzy NumPy a PyTorch,\n",
        "* algorytm wstecznej propagacji błędu w PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0AoiF16OTow"
      },
      "source": [
        "## PyTorch\n",
        "\n",
        "PyTorch to potężna biblioteka do uczenia maszynowego i głębokiego uczenia, stworzona przez Meta (dawniej Facebook).\n",
        "U podstaw jest to biblioteka implementująca operacje na tensorach (tablicach) na wzór NumPy, którego poszczególne elementy zostały zaprezentowane na poprzednich zajęciach.\n",
        "PyTorch oferuje dynamiczne podejście do tworzenia sieci neuronowych, co sprawia, że jest szczególnie popularny w środowisku badawczym i akademickim.\n",
        "\n",
        "Główne cechy PyTorch:\n",
        "- Natywna integracja z Python i łatwa współpraca z bibliotekami jak NumPy\n",
        "- Imperatywny styl programowania, który jest bardziej naturalny dla programistów Pythona\n",
        "- Dynamiczne grafy obliczeniowe (define-by-run), które ułatwiają debugowanie\n",
        "- Wsparcie dla CPU i GPU, z automatycznym przełączaniem między nimi\n",
        "- Rozbudowany ekosystem narzędzi i bibliotek (torchvision, torchaudio, torchtext)\n",
        "\n",
        "Aby rozpocząć pracę z PyTorch w Colab, wystarczy zaimportować bibliotekę:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ovUb2T9CbQv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "935f19f7-959b-4dd2-c2ed-2d24368b48c9"
      },
      "source": [
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.6.0+cu124\n",
            "CUDA available: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Jeśli nie korzystasz z Colab odwiedź https://pytorch.org/get-started/locally/ by dowiedzieć się jak zainstalować odpowiednią wersję dla swojego środowiska."
      ],
      "metadata": {
        "id": "3WuWss5Venrm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyOQAsNXECp0"
      },
      "source": [
        "## Używanie biblioteki PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBy3wd9GSXa0"
      },
      "source": [
        "PyTorch domyślnie używa tensorów (tablic) z API bardzo podobnym do NumPy, poznanego na poprzednich zajęciach.\n",
        "\n",
        "Poniżej przedstawione zostało porównanie wykonania tego samego zadania w NumPy oraz w PyTorch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jl7PpJ5OOVNj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64940ad9-19c0-498b-addb-764d17bd3e16"
      },
      "source": [
        "# Import biblioteki numpy\n",
        "import numpy as np\n",
        "\n",
        "# NumPy\n",
        "print(\"NumPy:\")\n",
        "x_np = np.array([[1., 2.], [3., 4.]])\n",
        "y_np = np.array([[5., 6.], [7., 8.]])\n",
        "z_np = np.multiply(x_np, y_np)\n",
        "print(f\"x:\\n{x_np}\")\n",
        "print(f\"y:\\n{y_np}\")\n",
        "print(f\"x * y:\\n{z_np}\")\n",
        "\n",
        "# PyTorch\n",
        "print(\"\\nPyTorch:\")\n",
        "x_torch = torch.tensor([[1., 2.], [3., 4.]])\n",
        "y_torch = torch.tensor([[5., 6.], [7., 8.]])\n",
        "z_torch = torch.mul(x_torch, y_torch)  # lub po prostu x_torch * y_torch\n",
        "print(f\"x:\\n{x_torch}\")\n",
        "print(f\"y:\\n{y_torch}\")\n",
        "print(f\"x * y:\\n{z_torch}\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NumPy:\n",
            "x:\n",
            "[[1. 2.]\n",
            " [3. 4.]]\n",
            "y:\n",
            "[[5. 6.]\n",
            " [7. 8.]]\n",
            "x * y:\n",
            "[[ 5. 12.]\n",
            " [21. 32.]]\n",
            "\n",
            "PyTorch:\n",
            "x:\n",
            "tensor([[1., 2.],\n",
            "        [3., 4.]])\n",
            "y:\n",
            "tensor([[5., 6.],\n",
            "        [7., 8.]])\n",
            "x * y:\n",
            "tensor([[ 5., 12.],\n",
            "        [21., 32.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "W obu przypadkach uzyskamy identyczny wynik, ale PyTorch oferuje dodatkowo:\n",
        "\n",
        "- Automatyczne obliczanie gradientów (autograd)\n",
        "- Łatwe przenoszenie obliczeń na GPU\n",
        "- Zachowanie informacji o grafie obliczeniowym\n",
        "- Pełną integrację z debuggerem Pythona\n",
        "\n",
        "Inne przykłady składni bardzo podobnej do NumPy:"
      ],
      "metadata": {
        "id": "kgniwGZHgQsF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Operacje są prawie identyczne jak w NumPy\n",
        "print(x_np + y_np)\n",
        "print(x_torch + y_torch)  # dodawanie\n",
        "\n",
        "print(x_np * y_np)\n",
        "print(x_torch * y_torch)  # mnożenie element-wise\n",
        "\n",
        "print(x_np @ y_np)\n",
        "print(x_torch @ y_torch)  # mnożenie macierzowe\n",
        "\n",
        "print(np.mean(x_np))\n",
        "print(torch.mean(x_torch))  # średnia\n",
        "\n",
        "print(np.sum(x_np, axis=0))\n",
        "print(torch.sum(x_torch, dim=0))  # suma wzdłuż wymiaru"
      ],
      "metadata": {
        "id": "eWLUWruQgPaJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa02dc05-c1e2-489b-be1e-c051c0cb24cb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 6.  8.]\n",
            " [10. 12.]]\n",
            "tensor([[ 6.,  8.],\n",
            "        [10., 12.]])\n",
            "[[ 5. 12.]\n",
            " [21. 32.]]\n",
            "tensor([[ 5., 12.],\n",
            "        [21., 32.]])\n",
            "[[19. 22.]\n",
            " [43. 50.]]\n",
            "tensor([[19., 22.],\n",
            "        [43., 50.]])\n",
            "2.5\n",
            "tensor(2.5000)\n",
            "[4. 6.]\n",
            "tensor([4., 6.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKGuFNGnTa9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b90c3243-d72a-41bc-db62-81f071113625"
      },
      "source": [
        "# Również w przypadku alokowania tablic/tensorów\n",
        "\n",
        "# Tablice zer\n",
        "arr_np = np.zeros([5, 5])\n",
        "arr_torch = torch.zeros(5, 5)\n",
        "\n",
        "# Tablice jedynek\n",
        "arr_np = np.ones([5, 5])\n",
        "arr_torch = torch.ones(5, 5)\n",
        "\n",
        "# Tablice ze zdefiniowanymi wartościami\n",
        "arr_np = np.array([1, 2, 3, 4, 5])\n",
        "arr_torch = torch.tensor([1, 2, 3, 4, 5])\n",
        "\n",
        "# Tablice z losowymi wartościami z rozkładu normalnego\n",
        "arr_np = np.random.normal(0, 1, [5, 5])\n",
        "arr_torch = torch.randn(5, 5)  # standardowy rozkład normalny (mean=0, std=1)\n",
        "# Alternatywnie, z konkretną średnią i odchyleniem:\n",
        "arr_torch = torch.normal(mean=0, std=1, size=(5, 5))\n",
        "\n",
        "print('NumPy array:\\n', arr_np, '\\n')\n",
        "print('PyTorch array:\\n', arr_torch, '\\n')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NumPy array:\n",
            " [[ 0.17818422  0.84055237  1.20854109  0.39828984  0.44522279]\n",
            " [ 0.95023135  1.12423204 -1.45238158  0.72496447 -0.20802212]\n",
            " [ 2.19365628 -1.15474592  1.12627519  1.17848003 -0.76819388]\n",
            " [ 0.4509379   0.67204709  2.13495947  1.14718028  0.44292757]\n",
            " [ 1.15775805 -0.46507032 -0.08259619 -0.12036391 -0.95856952]] \n",
            "\n",
            "PyTorch array:\n",
            " tensor([[ 0.0337, -1.4928, -0.7934,  0.2869,  2.5314],\n",
            "        [ 1.7664,  0.3410,  2.1835,  0.5812,  0.5130],\n",
            "        [ 0.3573,  1.0864,  1.1672,  1.5840, -0.2454],\n",
            "        [-0.1094,  0.6912, -1.1792,  0.2264,  0.1151],\n",
            "        [-1.9000, -1.7457, -1.5665, -0.8447,  0.8298]]) \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Warto zauważyć kilka różnic między NumPy a PyTorch:\n",
        "\n",
        "- W PyTorch często pomijamy \"nawiasy\" przy definiowaniu wymiarów - używamy po prostu `torch.zeros(5, 5)` zamiast `torch.zeros([5, 5])` lub `torch.zeros((5, 5))`. Czyli kolejne wymiary są przekazywane jako kolejne argumenty a nie obiekt o typie sekwencji.\n",
        "- Zamiast array używamy tensor do tworzenia tensorów ze zdefiniowanymi wartościami.\n",
        "Dla rozkładu normalnego PyTorch oferuje zarówno `torch.randn()` (dla standardowego rozkładu normalnego) jak i bardziej ogólną funkcję `torch.normal()`.\n",
        "- Operacje po konkretnych wymiarach używają argumentu `dim` zamiast `axis`.\n",
        "\n",
        "Wszystkie te tensory mogą być łatwo przeniesione na GPU poprzez dodanie metody `.cuda()` lub `.to('cuda')`, na przykład:\n"
      ],
      "metadata": {
        "id": "rT4hyM6yhZ-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    arr_torch = arr_torch.cuda()\n",
        "arr_torch"
      ],
      "metadata": {
        "id": "xaCtOIE5hZRB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd83ca1d-473e-4e52-ba9d-fa6a223c535c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0337, -1.4928, -0.7934,  0.2869,  2.5314],\n",
              "        [ 1.7664,  0.3410,  2.1835,  0.5812,  0.5130],\n",
              "        [ 0.3573,  1.0864,  1.1672,  1.5840, -0.2454],\n",
              "        [-0.1094,  0.6912, -1.1792,  0.2264,  0.1151],\n",
              "        [-1.9000, -1.7457, -1.5665, -0.8447,  0.8298]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Od teraz wszystkie operacje wykonywane na tym tensorze będą się automatycznie działy na GPU.\n",
        "\n",
        "**Ważne:** Jeśli dwa obiekty wchodzą ze sobą w interakcję muszą znajdować się na tym samym urządzeniu (albo oba na GPU, albo oba na CPU)."
      ],
      "metadata": {
        "id": "4YOWeRg-Fatp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDAxwKNDUMrH"
      },
      "source": [
        "### Ćwiczenie\n",
        "\n",
        "Zaalokuj tablicę samych zer o wymiarze 2 x 2. Następnie utwórz tablicę jedynek o tym samym wymiarze (wykorzystaj funkcję ones_like)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnt4cj4vWhms",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1049afc-f21b-4415-b70c-f692e3067982"
      },
      "source": [
        "# alokacja zer\n",
        "z = torch.zeros(2,2)\n",
        "\n",
        "# alokacja jedynek\n",
        "o = torch.ones_like(z)\n",
        "\n",
        "print(z)\n",
        "print(o)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0.],\n",
            "        [0., 0.]])\n",
            "tensor([[1., 1.],\n",
            "        [1., 1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIo9g-OZI-jv"
      },
      "source": [
        "### Zmienne \"uczone\" i autograd w PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZW2QB-HJDxu"
      },
      "source": [
        "Drugim filarem biblioteki PyTorch, po operacjach na tensorach, jest autograd.\n",
        "Autograd to kluczowa funkcjonalność, która umożliwia automatyczne obliczanie gradientów. Jest to fundamentalny mechanizm używany w uczeniu głębokich sieci neuronowych.\n",
        "\n",
        "W PyTorch możemy oznaczać tensory, które mają brać udział w liczeniu gradientów poprzez `requires_grad=True`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpLPMsD0JeB_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "926bd902-a933-412b-e804-4f3380760da6"
      },
      "source": [
        "x = torch.tensor(2.0, requires_grad=True)  # włączamy śledzenie gradientu\n",
        "#x = torch.tensor(2.0)  # to spowoduje błąd poniższego kodu\n",
        "print(f\"x = {x}\")\n",
        "print(f\"Czy x wymaga gradientu? {x.requires_grad}\")\n",
        "\n",
        "# Wykonujemy operacje matematyczne\n",
        "y = x * 2  # y = 2x\n",
        "z = y * y  # z = 4x²\n",
        "\n",
        "print(f\"\\ny = {y}\")\n",
        "print(f\"z = {z}\")\n",
        "\n",
        "# Obliczamy gradient dz/dx\n",
        "z.backward()\n",
        "\n",
        "# Sprawdzamy gradient (powinien być 16, bo dz/dx = 8x dla x=2)\n",
        "print(f\"\\nGradient dz/dx = {x.grad}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x = 2.0\n",
            "Czy x wymaga gradientu? True\n",
            "\n",
            "y = 4.0\n",
            "z = 16.0\n",
            "\n",
            "Gradient dz/dx = 16.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Alternatywną metodą zdefiniowana zmiennej wymagającej liczenia gradientu\n",
        "# jest użycie klasy wrappującej parametr sieci neuronowej\n",
        "import torch.nn as nn\n",
        "\n",
        "x = nn.Parameter(torch.tensor(2.0))\n",
        "print(f\"x = {x}\")\n",
        "print(f\"Czy x wymaga gradientu? {x.requires_grad}\")\n",
        "\n",
        "# Wykonujemy operacje matematyczne\n",
        "y = x * 2  # y = 2x\n",
        "z = y * y  # z = 4x²\n",
        "\n",
        "print(f\"\\ny = {y}\")\n",
        "print(f\"z = {z}\")\n",
        "\n",
        "# Obliczamy gradient dz/dx\n",
        "z.backward()\n",
        "\n",
        "# Sprawdzamy gradient (powinien być 16, bo dz/dx = 8x dla x=2)\n",
        "print(f\"\\nGradient dz/dx = {x.grad}\")"
      ],
      "metadata": {
        "id": "uDM2kQrjoItG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67588dc0-a84a-4259-dcb2-37aa1751ffa4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x = Parameter containing:\n",
            "tensor(2., requires_grad=True)\n",
            "Czy x wymaga gradientu? True\n",
            "\n",
            "y = 4.0\n",
            "z = 16.0\n",
            "\n",
            "Gradient dz/dx = 16.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aby potem wyzerować gradient ale nie usuwać samej zmiennej możemy ustawić go na `None`."
      ],
      "metadata": {
        "id": "Aznpgq5JkSyy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad = None"
      ],
      "metadata": {
        "id": "eXGr0lNpkP_V"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Sh_nuvpLfJu"
      },
      "source": [
        "### Funkcje aktywacji i straty w PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbUbMgx0LkXd"
      },
      "source": [
        "PyTorch jest biblioteką przygotowaną do **Machine Learningu**. Wiele operacji bardzo często się powtarza w wielu zagadnieniach, np. funkcje aktywacji, funkcje straty, metryki, rodzaje warstw (operacji), itp. Stąd, w bilbiotece tej możemy znaleźć już gotowe komponenty, które można z łatwością wykorzystać (więcej o tym na kolejnych zajęciach).\n",
        "\n",
        "Poniżej zaprezentowane zostały gotowe funkcje aktywacji oraz straty."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_Z84f4NMI0_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c76b9c6b-d091-41ba-efb0-7179536a873f"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F  # funkcje aktywacji i inne operacje\n",
        "\n",
        "# 1. Funkcje aktywacji\n",
        "# Możemy używać ich na dwa sposoby:\n",
        "# a) Jako warstwy (zalecane w modelach, o warstwach więcej na kolejnych zajęciach):\n",
        "activation_layers = {\n",
        "    'ReLU': nn.ReLU(),\n",
        "    'LeakyReLU': nn.LeakyReLU(negative_slope=0.1),\n",
        "    'Sigmoid': nn.Sigmoid(),\n",
        "    'Tanh': nn.Tanh(),\n",
        "    'ELU': nn.ELU(alpha=1.0),\n",
        "    'SELU': nn.SELU(),\n",
        "    'Softmax': nn.Softmax(dim=1),\n",
        "    'LogSoftmax': nn.LogSoftmax(dim=1)\n",
        "}\n",
        "\n",
        "# b) Jako funkcje (przydatne w obliczeniach doraźnych):\n",
        "x = torch.randn(2, 3)\n",
        "print(\"Dane wejściowe:\")\n",
        "print(x)\n",
        "\n",
        "print(\"\\nRóżne funkcje aktywacji:\")\n",
        "print(\"ReLU:\", F.relu(x))\n",
        "print(\"Sigmoid:\", torch.sigmoid(x))\n",
        "print(\"Tanh:\", torch.tanh(x))\n",
        "print(\"Softmax:\", F.softmax(x, dim=1))\n",
        "\n",
        "# 2. Funkcje straty (loss functions)\n",
        "loss_functions = {\n",
        "    'MSE': nn.MSELoss(),  # Mean Squared Error - regresja\n",
        "    'BCE': nn.BCELoss(),  # Binary Cross Entropy - klasyfikacja binarna\n",
        "    'CrossEntropy': nn.CrossEntropyLoss(),  # klasyfikacja wieloklasowa\n",
        "    'L1': nn.L1Loss(),  # Mean Absolute Error\n",
        "    'Huber': nn.HuberLoss(),  # połączenie MSE i L1\n",
        "    'KLDiv': nn.KLDivLoss(),  # Kullback-Leibler Divergence\n",
        "}\n",
        "\n",
        "# Przykład użycia funkcji straty:\n",
        "# Regresja\n",
        "x = torch.randn(3, 1)  # predykcje\n",
        "y = torch.randn(3, 1)  # wartości rzeczywiste\n",
        "mse_loss = nn.MSELoss()(x, y)\n",
        "print(f\"\\nMSE Loss: {mse_loss.item()}\")\n",
        "\n",
        "# Klasyfikacja\n",
        "predictions = torch.randn(3, 5)  # logity dla 5 klas\n",
        "targets = torch.tensor([1, 0, 4])  # prawidłowe klasy\n",
        "ce_loss = nn.CrossEntropyLoss()(predictions, targets)\n",
        "print(f\"Cross Entropy Loss: {ce_loss.item()}\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dane wejściowe:\n",
            "tensor([[-0.3630,  0.6588,  0.0410],\n",
            "        [ 0.1396, -0.4058, -0.5735]])\n",
            "\n",
            "Różne funkcje aktywacji:\n",
            "ReLU: tensor([[0.0000, 0.6588, 0.0410],\n",
            "        [0.1396, 0.0000, 0.0000]])\n",
            "Sigmoid: tensor([[0.4102, 0.6590, 0.5103],\n",
            "        [0.5348, 0.3999, 0.3604]])\n",
            "Tanh: tensor([[-0.3479,  0.5776,  0.0410],\n",
            "        [ 0.1387, -0.3849, -0.5179]])\n",
            "Softmax: tensor([[0.1895, 0.5266, 0.2839],\n",
            "        [0.4831, 0.2801, 0.2368]])\n",
            "\n",
            "MSE Loss: 0.30484625697135925\n",
            "Cross Entropy Loss: 2.22334361076355\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxSdsMUWD9ci"
      },
      "source": [
        "## Kompilacja grafów obliczeniowych\n",
        "\n",
        "W PyTorch również istnieje możliwość optymalizacji kodu poprzez kompilację grafów obliczeniowych, używając torch.jit (znanego jako TorchScript) oraz torch.compile() (wprowadzonego w PyTorch 2.0). Zaletą ich używania jest to, że pozwalają one używać podzbioru normalnego Pythona."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ets0mmTSUVsK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca8c48c7-77ea-46d0-f077-8a72dcfacb8a"
      },
      "source": [
        "import torch\n",
        "import time\n",
        "\n",
        "# 1. Używając TorchScript (@torch.jit.script)\n",
        "@torch.jit.script\n",
        "def scripted_fn(x, y):\n",
        "    for i in range(100):\n",
        "        x = x + y\n",
        "    return x\n",
        "\n",
        "# 2. Używając torch.compile (PyTorch 2.0+)\n",
        "@torch.compile\n",
        "def compiled_fn(x, y):\n",
        "    for i in range(100):\n",
        "        x = x + y\n",
        "    return x\n",
        "\n",
        "# 3. Zwykła funkcja Pythona dla porównania\n",
        "def regular_fn(x, y):\n",
        "    for i in range(100):\n",
        "        x = x + y\n",
        "    return x\n",
        "\n",
        "# Przygotowanie danych\n",
        "x = torch.randn(1000, 1000)\n",
        "y = torch.randn(1000, 1000)\n",
        "\n",
        "# Porównanie wydajności\n",
        "def benchmark(fn, name, x, y, warmup=1, runs=5):\n",
        "    # Warmup\n",
        "    for _ in range(warmup):\n",
        "        _ = fn(x, y)\n",
        "\n",
        "    # Pomiar czasu\n",
        "    start = time.time()\n",
        "    for _ in range(runs):\n",
        "        _ = fn(x, y)\n",
        "    end = time.time()\n",
        "\n",
        "    print(f\"{name}: {(end-start)/runs:.4f} sekund na iterację\")\n",
        "\n",
        "# Uruchamiamy benchmark\n",
        "print(\"Porównanie wydajności:\")\n",
        "benchmark(regular_fn, \"Zwykła funkcja\", x, y)\n",
        "benchmark(scripted_fn, \"TorchScript\", x, y)\n",
        "benchmark(compiled_fn, \"Torch Compile\", x, y)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Porównanie wydajności:\n",
            "Zwykła funkcja: 0.0572 sekund na iterację\n",
            "TorchScript: 0.0599 sekund na iterację\n",
            "Torch Compile: 0.0083 sekund na iterację\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ważne uwagi:\n",
        "\n",
        "TorchScript (`@torch.jit.script`):\n",
        "\n",
        "- Kompiluje (szybko) kod Python do pośredniej reprezentacji\n",
        "- Pozwala na eksport modelu do formatu niezależnego od Pythona\n",
        "- Ma pewne ograniczenia składniowe i typowanie\n",
        "- Dobry do produkcyjnego wdrażania modeli\n",
        "\n",
        "\n",
        "Torch Compile (`@torch.compile`):\n",
        "\n",
        "- Nowsza metoda wprowadzona w PyTorch 2.0\n",
        "- Często daje lepszą optymalizację\n",
        "- Łatwiejsza w użyciu niż TorchScript\n",
        "- Zachowuje więcej funkcjonalności Pythona\n",
        "- Wymaga dłuższej kompilacji przy pierwszym uruchomienu funkcji.\n",
        "\n",
        "Generalnie od wersji 2.0 zaleca się używanie Torch Compile.\n",
        "Zarówno TorchScript jak i Torch Compile utrudniają debugowanie."
      ],
      "metadata": {
        "id": "v3VHiaonmtuR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQ-XK5nUEUWd"
      },
      "source": [
        "## Algorytm wstecznej propagacji błędu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWWYxKRmguWl"
      },
      "source": [
        "Do wyprowadzenia wzorów na aktualizację zmiennych w algorytmie niezbędna jest znajomość zagadnień **chain-rule** oraz **multivariable chain-rule**. Poza podstawową wiedzą z matematyki są to jedyne niezbędne zagadnienia.\n",
        "\n",
        "### Chain-rule & Multivariable Chain-rule\n",
        "\n",
        "Poniżej zostały zaprezentowane zagadnienia **chain-rule** oraz **multivariable chain-rule** w sposób intuicyjny (i graficzny). Poniższa notacja została zaczerpnięta z sieci neuronowych, aby skojarzyć związek z algorytmem **backpropagation**.\n",
        "\n",
        "#### Chain-rule\n",
        "\n",
        "Jest to reguła opierająca się na pochodnej funkcji złożonej. W graficzny sposób można to pokazać następująco:\n",
        "\n",
        "![chain-rule](https://drive.google.com/uc?id=1S8pSUrrOzoisKr8d8YJaQHNLYgEq2dpk)\n",
        "\n",
        "Gdzie $L$ to pewna funkcja dla której liczymy pochodną względem zmiennej $w$. Jak widać, $L$ zależy od $y$, który zależy od $z$ który dopiero zależy bezpośrednio od $w$. Zależności tworzą łańcuch, skąd pochodzi nazwa **chain-rule**.\n",
        "\n",
        "#### Multivariable Chain-rule\n",
        "\n",
        "W przypadku gdy \"łańcuch\" w pewnym momencie się rozgałęzia, dalej możemy korzystać z zasady **chain-rule**, jednak tym razem przybiera ona formę nieco inną (lecz dalej intuicyjną):\n",
        "\n",
        "![multivariable-chain-rule](https://drive.google.com/uc?id=13WH6JXd_5rnTkyToKrBcCZDPdUXCqoT8)\n",
        "\n",
        "W tym przypadku, pochodna funkcji $L$ po zmiennej $z$ wyrażona jest jako suma pochodnych funkcji złożonych odpowiednio $y_1$ oraz $y_2$. Nic nie stoi na przeszkodzie, żeby stosować tę regułę także w sytuacji większej liczby rozgałęzień niż dwa, po prostu zawsze się dodaje pochodne z osobnych \"ścieżek\".\n",
        "\n",
        "<!-- \\\\ %ib: chyba nie jest to tak dobry przykład, jak myślałem...\n",
        "\n",
        "##### **Przykład:**\n",
        "Załóżmy, że mamy funkcję $f(x) = x\\cdot ln(x)$. Możemy ją przedstawić jako $f(x) = g(x)\\cdot h(x)$, gdzie $g(x)=x$ i $h(x)=ln(x)$. Korzystając z multivariable chain-rule możemy wyprowadzić:\n",
        "\n",
        "$$\\frac{\\partial f(x)}{\\partial x} = \\frac{\\partial g(x)h(x)}{\\partial x} = \\frac{\\partial g(x)h(x)}{\\partial g(x)} \\frac{\\partial g(x)}{\\partial x} + \\frac{\\partial g(x)h(x)}{\\partial h(x)} \\frac{\\partial h(x)}{\\partial x} = h(x)\\cdot 1 + g(x)\\cdot \\frac{1}{x} = ln(x) + 1$$\n",
        "\n",
        "Zwróć uwagę, że wzór na pochodną iloczynu jest szczególnym przypadkiem multivariable chain-rule:\n",
        "$$(g(x)h(x))' = g'(x)h(x) + g(x)h'(x) = 1\\cdot ln(x) + x\\cdot\\frac{1}{x} = ln(x) + 1$$ -->\n",
        "\n",
        "\\\\\n",
        "\n",
        "### Backpropagation\n",
        "\n",
        "Algorytm **wstecznej propagacji błędu** jest właściwie jedynie rozwinięciem **reguły delta/uczenia za pomocą metody spadku wzdłóż gradientu**, która została przedstawiona na pierwszych zajęciach. Podobnie jak poprzednio, uczenie neuronów opisane jest w następujący sposób:\n",
        "\n",
        "$$w_i' = w_i - \\mu \\frac{\\partial L}{\\partial w_i} $$\n",
        "\n",
        "Jedyną różnicą jest sposób w jaki się wylicza gradient. Nawiązując do powyższych, krótkich wyjaśnień **chain-rule**, w podobny sposób należy traktować sieci neuronowe.\n",
        "\n",
        "**Dla warstwy wyjściowej** gradient można przedstawić następująco:\n",
        "\n",
        "$$\\frac{\\partial L}{\\partial w_{ij}} = \\frac{\\partial L}{\\partial y_j} \\frac{\\partial y_j}{\\partial w_{ji}} = \\frac{\\partial L}{\\partial y_j} \\frac{\\partial y_j}{\\partial z_j}\\frac{\\partial z_j}{\\partial w_{ji}}$$\n",
        "\n",
        "Albo inaczej:\n",
        "\n",
        "$$\\frac{\\partial L}{\\partial w_{ij}} = \\delta_j o_i$$\n",
        "\n",
        "gdzie $w_{ji}$ oznacza wagę połączenia pomiędzy neuronem $j$ a $i$, $\\delta_j = \\frac{\\partial L}{\\partial y_j} \\frac{\\partial y_j}{\\partial z_j}$ oraz $o_i$ to wyjście z neuronu $i$ (dla neuronów w pierwszej warstwie będą to wejścia do sieci, natomiast w kolejnych warstwach będą to wyjścia z neuronów poprzednich).\n",
        "\n",
        "**Dla warstwy ukrytej** sprawa nieco bardziej się komplikuje. Weźmy jako przykład następującą prostą sieć z 2 warstwami i łącznie 3 neuronami:\n",
        "\n",
        "![simplenet](https://drive.google.com/uc?id=1NXHjEWUX3gAbn8NnHh9eGWdQaSNDavKW)\n",
        "\n",
        "Dla warstwy wyjściowej możemy skorzystać jedynie z **chain-rule** uzyskując następujące równanie:\n",
        "\n",
        "$$\\frac{\\partial L}{\\partial w^{2}_{i}} = \\frac{\\partial L}{\\partial y^{2}_i} \\frac{\\partial y^{2}_i}{\\partial z^{2}_i}\\frac{\\partial z^{2}_i}{\\partial w^{2}_{i}} = \\delta^2_i y^1_1$$\n",
        "\n",
        "Przy obliczaniu gradientu dla warstwy ukrytej najpierw stosujemy  proste **chain-rule**:\n",
        "\n",
        "$$\\frac{\\partial L}{\\partial w^{1}_{1}} = \\frac{\\partial L}{\\partial y^{1}_1} \\frac{\\partial y^{1}_1}{\\partial z^{1}_1}\\frac{\\partial z^{1}_1}{\\partial w^{1}_{1}}$$\n",
        "\n",
        "W tym momencie można zauważyć, że jest możliwe zastosowanie **multivariable chain-rule** do wyrażenia $\\frac{\\partial L}{\\partial y^{1}_1}$ (ponieważ $L$ nie zależy bezpośrednio od $y^{1}_1$, oraz występuje \"rozgałęzienie\" od $y$):\n",
        "\n",
        "$$\\frac{\\partial L}{\\partial y^{1}_1} = \\frac{\\partial L}{\\partial y^{2}_1} \\frac{\\partial y^{2}_1}{\\partial y^{1}_1} + \\frac{\\partial L}{\\partial y^{2}_2} \\frac{\\partial y^{2}_2}{\\partial y^{1}_1} = \\frac{\\partial L}{\\partial y^{2}_1} \\frac{\\partial y^{2}_1}{\\partial z^{2}_1}\\frac{\\partial z^{2}_1}{\\partial y^{1}_1} + \\frac{\\partial L}{\\partial y^{2}_2} \\frac{\\partial y^{2}_2}{\\partial z^{2}_2} \\frac{\\partial z^{2}_2}{\\partial y^{1}_1}$$\n",
        "\n",
        "Tu można zauważyć, że $\\frac{\\partial z^{2}_2}{\\partial y^{1}_1} = w^2_2$ oraz $\\frac{\\partial L}{\\partial y^{2}_2} \\frac{\\partial y^{2}_2}{\\partial z^{2}_2} = \\delta^2_2$ (przy założeniu liniowej funkcji aktywacji), co daje nam końcowo:\n",
        "\n",
        "$$\\frac{\\partial L}{\\partial y^{1}_1} = \\sum_i^2 \\delta^2_i w^2_i$$\n",
        "\n",
        "Podstawiając do wcześniejszego wzoru otrzymujemy:\n",
        "\n",
        "$$\\frac{\\partial L}{\\partial w^{1}_{1}} = \\sum_i^2 \\delta^2_i w^2_i \\frac{\\partial y^{1}_1}{\\partial z^{1}_1}\\frac{\\partial z^{1}_1}{\\partial w^{1}_{1}}$$\n",
        "\n",
        "Ostatecznie, za pomocą następujących podstawień możemy sprowadzić powyższe równanie do wersji spójnej z gradientem dla warstwy wyjściowej:\n",
        "\n",
        "$$\\sum_i^2 \\delta^2_i w^2_i \\frac{\\partial y^{1}_1}{\\partial z^{1}_1} = \\delta^1_1$$\n",
        "$$\\frac{\\partial z^{1}_1}{\\partial w^{1}_{1}} = x$$\n",
        "$$\\frac{\\partial L}{\\partial w^{1}_{1}} = \\delta^1_1 x$$\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Podsumowując, zarówno dla warstwy wyjściowej jak i warstwy ukrytej gradienty możemy przedstawić w tej samej formie, składającej się z **błędu neuronu** ($\\delta$), oraz wyjścia z poprzedniej warstwy ($y$ lub $x$).\n",
        "\n",
        "$$\\frac{\\partial L}{\\partial w^{i}_{j}} = \\delta^i_j o^{(i-1)}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJKj35Ykeqzn"
      },
      "source": [
        "### Automatyczna wsteczna propagacja błędu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DA3PfOaWcE9q"
      },
      "source": [
        "Można rozpatrywać różne algorytmy optymalizacji działające na bazie algorytmu wstecznej propagacji błędu, który sam w sobie jest po prostu pewną strategią przydziału informacji o błędzie do zmiennych, a to jak na tej podstawie będziemy je modyfikować pozostaje otwartą kwestią. Jednym z prostszych optymalizatorów jest **Stochastic Gradient Descent** (SGD). Różni się od klasycznego Gradient Descent tym, że w jego przypadku gradient liczony jest w każdej iteracji dla losowo wybranego podzbioru danych uczących zamiast dla wszystkich. Innym popularnym optymalizatorem jest **ADAM** (od \"adaptive moment estimation\"), który każdą zmienną modyfikuje na podstawie jej statystycznych momentów (rzędu 1 i 2) gradientu w pewnym oknie czasowym. Ogólnie konstruowanie optymalizatorów opartych na gradiencie to cała osobna obszerna dziedzina naukowa. Przykładowo, na niektórych problemach ADAM ma znacznie szybszą zbieżność niż SGD, ale na innych zauważalnie gorzej radzi sobie z uogólnieniem wiedzy i w efekcie błąd na zbiorze testowym jest większy.\n",
        "\n",
        "Poniżej zaprezentowany został algorytm wstecznej propagacji błędu zaimplementowany w bibliotece PyTorch. Przykład składa się z definicji sieci neuronowej, funkcji straty oraz jednego kroku uczenia przy użyciu gotowego optymalizatora."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyUufSiJEgUH"
      },
      "source": [
        "### Prosta sieć neuronowa"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ra9SuKPAE1wU"
      },
      "source": [
        "import torch\n",
        "\n",
        "# zmienne uczone pojedynczego neuronu, który na wejście otrzymuje wektor 10 liczb\n",
        "w = torch.nn.Parameter(torch.ones(10, 1))\n",
        "b = torch.nn.Parameter(torch.ones(1))\n",
        "\n",
        "# definicja 1-warstwowej sieci neuronowej z funkcją aktywacji relu\n",
        "# (@ jest również aliasem do torch.matmul() w PyTorch)\n",
        "def network(x):\n",
        "    return torch.relu(x @ w + b)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c97P9HzdEnID"
      },
      "source": [
        "### Funkcja straty"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAaD7jSpyUmk"
      },
      "source": [
        "W poniższym kodzie torch.mean to odpowiednik np.mean i działa tak samo. Jako drugi argument można podać wymiar, po którym liczona będzie średnia (domyślnie liczona jest z wszystkich wymiarów). Jeżeli chodzi o *reduce* (nazywane również *fold*) w nazwie, to jest to schemat obliczeń często wykorzystywany w paradygmacie programowania funkcyjnego, i polega na rekurencyjnym przechodzeniu przez strukturę przy jednoczesnym konstruowaniu nowej, w pewnym sensie redukując tę strukturę (np. macierz liczb) do nowej struktury (np. liczby)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqwbPkFiE2UZ"
      },
      "source": [
        "# mean squared error jako funkcja straty\n",
        "def loss_fn(y_pred, y_true):\n",
        "   return torch.mean((y_pred - y_true) ** 2)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWv0Fg36Epug"
      },
      "source": [
        "### Obliczenie gradientu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ejn6iHQE2kw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb2fb31a-596d-4ee4-b46e-2f55d82af4d1"
      },
      "source": [
        "# wsteczna propagacja błędu obsługiwana jest przez optymalizatory dostępne w PyTorch\n",
        "optimizer = torch.optim.Adam([w, b], lr=0.001)\n",
        "#optimizer = torch.optim.SGD([w, b], lr=0.001)  # a tutaj Stochastic Gradient Descent\n",
        "\n",
        "# wejście oraz pożądane wyjście z sieci neuronowej\n",
        "x = torch.rand(32, 10)\n",
        "y_true = torch.ones(32, 1)\n",
        "\n",
        "# W PyTorch nie potrzebujemy GradientTape, gradienty są obliczane automatycznie\n",
        "# Najpierw zerujemy gradienty z poprzedniej iteracji\n",
        "optimizer.zero_grad()\n",
        "\n",
        "# Forward pass\n",
        "y_pred = network(x)\n",
        "l = loss_fn(y_pred, y_true)\n",
        "\n",
        "# Backward pass - obliczenie gradientów\n",
        "l.backward()\n",
        "\n",
        "# Aktualizacja parametrów przez optymalizator\n",
        "optimizer.step()\n",
        "\n",
        "# Wyświetlenie gradientów\n",
        "print([w.grad, b.grad])  # gradienty są przechowywane w .grad"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[5.7925],\n",
            "        [5.6138],\n",
            "        [5.5791],\n",
            "        [4.4633],\n",
            "        [4.6250],\n",
            "        [6.1746],\n",
            "        [4.8555],\n",
            "        [4.8852],\n",
            "        [4.7506],\n",
            "        [5.4124]]), tensor([10.0765])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idPo7uRZexv9"
      },
      "source": [
        "#### Zadanie 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFKkCPhgezMI"
      },
      "source": [
        "Korzystając z wiedzy z tych i poprzednich zajęć stwórz sieć neuronową składającą się z 3 warstw o rozmiarach odpowiednio 1, 2, 1. Niech w pierwszej i drugiej warstwie funkcją aktywacji będzie sigmoid a wyjście z sieci funkcją liniową. Skorzystaj z funkcji straty MSE.\n",
        "\n",
        "Naucz sieć neuronową, aby wykonywała funkcję **sinus** (wejściem sieci niech będzie kąt wyrażony w radianach). Trening sieci powinien polegać na wykonaniu algorytmu wstecznej propagacji błędu **ITERS** razy. Każda iteracja niech przetwarza **batch** danych o rozmiarze 16. Do nauki sieci wykorzystaj algorytm **Stochastic Gradient Descent** (`torch.optim.SGD`).\n",
        "\n",
        "W celu optymalizacji szybkości działania, dodaj do definicji modelu adnotację @torch.compile.\n",
        "\n",
        "Przy domyślnych parametrach wyjdzie zasadniczo linia prosta na zerze. Poeksperymentuj z prędkością uczenia i liczbą iteracji, by poprawić ten rezultat. Sprawdź, jak działa ReLU dla tego problemu. Dla sigmoidalnej funkcji aktywacji rezultat powinien przypominać następujący wykres:\n",
        "\n",
        "![alt text](https://www.cs.put.poznan.pl/ibladek/students/eio/img/expected_sine.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "ITERS = 30000  # ilość iteracji\n",
        "LR = 0.020  # prędkość uczenia\n",
        "\n",
        "# pierwsza warstwa\n",
        "w1 = torch.nn.Parameter(torch.rand(1, 2))\n",
        "b1 = torch.nn.Parameter(torch.rand(2))\n",
        "\n",
        "# druga warstwa\n",
        "w2 = torch.nn.Parameter(torch.rand(2, 2))\n",
        "b2 = torch.nn.Parameter(torch.rand(2))\n",
        "\n",
        "# trzecia warstwa\n",
        "w3 = torch.nn.Parameter(torch.rand(2, 1))\n",
        "b3 = torch.nn.Parameter(torch.rand(1))\n",
        "\n",
        "def layer_1(x):\n",
        "    return torch.sigmoid(x @ w1 + b1)\n",
        "\n",
        "def layer_2(x):\n",
        "    return torch.sigmoid(x @ w2 + b2)\n",
        "\n",
        "def layer_3(x):\n",
        "    return x @ w3 + b3\n",
        "\n",
        "@torch.compile\n",
        "def network(x):  # obliczanie wyjścia sieci\n",
        "    x = layer_1(x)\n",
        "    x = layer_2(x)\n",
        "    x = layer_3(x)\n",
        "    return x\n",
        "\n",
        "all_parameters = [w1, b1, w2, b2, w3, b3]\n",
        "optimizer = torch.optim.SGD(all_parameters, lr=LR)\n",
        "loss_fn = torch.nn.MSELoss()\n",
        "\n",
        "for i in range(ITERS):\n",
        "    # dane wejściowe\n",
        "    x = torch.rand(16, 1) * (2 * math.pi)\n",
        "    y_true = torch.sin(x)  # dane uczące (ground truth)\n",
        "    x = (x - math.pi) / (2 * math.pi) # przeskalowanie danych do [-1, 1]\n",
        "\n",
        "    # zerowanie gradientów\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # forward pass\n",
        "    y_pred = network(x)\n",
        "\n",
        "    # obliczanie błędu\n",
        "    loss = loss_fn(y_pred, y_true)\n",
        "\n",
        "    # backward pass\n",
        "    loss.backward()\n",
        "\n",
        "    # aktualizacja wag\n",
        "    optimizer.step()\n",
        "\n",
        "    if i % 100 == 0:\n",
        "        print(f'Iteracja {i}, Loss: {loss.item():.4f}')\n",
        "\n",
        "# testowanie sieci na zbiorze testowym\n",
        "x = torch.linspace(0, 2 * math.pi, 100).reshape(-1, 1)\n",
        "y_test = torch.sin(x)\n",
        "x = (x - math.pi) / (2 * math.pi)\n",
        "with torch.no_grad():  # wyłączamy obliczanie gradientów podczas testowania\n",
        "    y_pred = network(x)\n",
        "\n",
        "# konwersja do numpy dla matplotlib\n",
        "x = x.numpy()\n",
        "y_test = y_test.numpy()\n",
        "y_pred = y_pred.numpy()\n",
        "\n",
        "plt.plot(x, y_test, label='sin(x)')\n",
        "plt.plot(x, y_pred, label='network(x)')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-yoSVpH5pVvo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d6de1a82-bdf3-4f0b-8cd8-f48689338b79"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteracja 0, Loss: 1.4785\n",
            "Iteracja 100, Loss: 0.2072\n",
            "Iteracja 200, Loss: 0.4677\n",
            "Iteracja 300, Loss: 0.4902\n",
            "Iteracja 400, Loss: 0.5979\n",
            "Iteracja 500, Loss: 0.4622\n",
            "Iteracja 600, Loss: 0.4865\n",
            "Iteracja 700, Loss: 0.4544\n",
            "Iteracja 800, Loss: 0.6875\n",
            "Iteracja 900, Loss: 0.5171\n",
            "Iteracja 1000, Loss: 0.3893\n",
            "Iteracja 1100, Loss: 0.5035\n",
            "Iteracja 1200, Loss: 0.3571\n",
            "Iteracja 1300, Loss: 0.3958\n",
            "Iteracja 1400, Loss: 0.4559\n",
            "Iteracja 1500, Loss: 0.4600\n",
            "Iteracja 1600, Loss: 0.4422\n",
            "Iteracja 1700, Loss: 0.5013\n",
            "Iteracja 1800, Loss: 0.3302\n",
            "Iteracja 1900, Loss: 0.5176\n",
            "Iteracja 2000, Loss: 0.4745\n",
            "Iteracja 2100, Loss: 0.4611\n",
            "Iteracja 2200, Loss: 0.5164\n",
            "Iteracja 2300, Loss: 0.5358\n",
            "Iteracja 2400, Loss: 0.3418\n",
            "Iteracja 2500, Loss: 0.4319\n",
            "Iteracja 2600, Loss: 0.4373\n",
            "Iteracja 2700, Loss: 0.4008\n",
            "Iteracja 2800, Loss: 0.4102\n",
            "Iteracja 2900, Loss: 0.4863\n",
            "Iteracja 3000, Loss: 0.5687\n",
            "Iteracja 3100, Loss: 0.4347\n",
            "Iteracja 3200, Loss: 0.4918\n",
            "Iteracja 3300, Loss: 0.5070\n",
            "Iteracja 3400, Loss: 0.4910\n",
            "Iteracja 3500, Loss: 0.6332\n",
            "Iteracja 3600, Loss: 0.6440\n",
            "Iteracja 3700, Loss: 0.4889\n",
            "Iteracja 3800, Loss: 0.5011\n",
            "Iteracja 3900, Loss: 0.5945\n",
            "Iteracja 4000, Loss: 0.4727\n",
            "Iteracja 4100, Loss: 0.5203\n",
            "Iteracja 4200, Loss: 0.4615\n",
            "Iteracja 4300, Loss: 0.4846\n",
            "Iteracja 4400, Loss: 0.3657\n",
            "Iteracja 4500, Loss: 0.4213\n",
            "Iteracja 4600, Loss: 0.5442\n",
            "Iteracja 4700, Loss: 0.4813\n",
            "Iteracja 4800, Loss: 0.5273\n",
            "Iteracja 4900, Loss: 0.6278\n",
            "Iteracja 5000, Loss: 0.4671\n",
            "Iteracja 5100, Loss: 0.3191\n",
            "Iteracja 5200, Loss: 0.4547\n",
            "Iteracja 5300, Loss: 0.3107\n",
            "Iteracja 5400, Loss: 0.5011\n",
            "Iteracja 5500, Loss: 0.5432\n",
            "Iteracja 5600, Loss: 0.4174\n",
            "Iteracja 5700, Loss: 0.5269\n",
            "Iteracja 5800, Loss: 0.4498\n",
            "Iteracja 5900, Loss: 0.5041\n",
            "Iteracja 6000, Loss: 0.4916\n",
            "Iteracja 6100, Loss: 0.5904\n",
            "Iteracja 6200, Loss: 0.5931\n",
            "Iteracja 6300, Loss: 0.5161\n",
            "Iteracja 6400, Loss: 0.4640\n",
            "Iteracja 6500, Loss: 0.5417\n",
            "Iteracja 6600, Loss: 0.4057\n",
            "Iteracja 6700, Loss: 0.3116\n",
            "Iteracja 6800, Loss: 0.6020\n",
            "Iteracja 6900, Loss: 0.4724\n",
            "Iteracja 7000, Loss: 0.4898\n",
            "Iteracja 7100, Loss: 0.4808\n",
            "Iteracja 7200, Loss: 0.7146\n",
            "Iteracja 7300, Loss: 0.3471\n",
            "Iteracja 7400, Loss: 0.4035\n",
            "Iteracja 7500, Loss: 0.5407\n",
            "Iteracja 7600, Loss: 0.4835\n",
            "Iteracja 7700, Loss: 0.5361\n",
            "Iteracja 7800, Loss: 0.6853\n",
            "Iteracja 7900, Loss: 0.4482\n",
            "Iteracja 8000, Loss: 0.4562\n",
            "Iteracja 8100, Loss: 0.4627\n",
            "Iteracja 8200, Loss: 0.6667\n",
            "Iteracja 8300, Loss: 0.5498\n",
            "Iteracja 8400, Loss: 0.4202\n",
            "Iteracja 8500, Loss: 0.3790\n",
            "Iteracja 8600, Loss: 0.5511\n",
            "Iteracja 8700, Loss: 0.5479\n",
            "Iteracja 8800, Loss: 0.5434\n",
            "Iteracja 8900, Loss: 0.5564\n",
            "Iteracja 9000, Loss: 0.4913\n",
            "Iteracja 9100, Loss: 0.5342\n",
            "Iteracja 9200, Loss: 0.5664\n",
            "Iteracja 9300, Loss: 0.5506\n",
            "Iteracja 9400, Loss: 0.4322\n",
            "Iteracja 9500, Loss: 0.5471\n",
            "Iteracja 9600, Loss: 0.4838\n",
            "Iteracja 9700, Loss: 0.3235\n",
            "Iteracja 9800, Loss: 0.4170\n",
            "Iteracja 9900, Loss: 0.5472\n",
            "Iteracja 10000, Loss: 0.4418\n",
            "Iteracja 10100, Loss: 0.5150\n",
            "Iteracja 10200, Loss: 0.4466\n",
            "Iteracja 10300, Loss: 0.5156\n",
            "Iteracja 10400, Loss: 0.5572\n",
            "Iteracja 10500, Loss: 0.3578\n",
            "Iteracja 10600, Loss: 0.5650\n",
            "Iteracja 10700, Loss: 0.3333\n",
            "Iteracja 10800, Loss: 0.3586\n",
            "Iteracja 10900, Loss: 0.5435\n",
            "Iteracja 11000, Loss: 0.3981\n",
            "Iteracja 11100, Loss: 0.4121\n",
            "Iteracja 11200, Loss: 0.1957\n",
            "Iteracja 11300, Loss: 0.3974\n",
            "Iteracja 11400, Loss: 0.3837\n",
            "Iteracja 11500, Loss: 0.3169\n",
            "Iteracja 11600, Loss: 0.3383\n",
            "Iteracja 11700, Loss: 0.2003\n",
            "Iteracja 11800, Loss: 0.1823\n",
            "Iteracja 11900, Loss: 0.2374\n",
            "Iteracja 12000, Loss: 0.2520\n",
            "Iteracja 12100, Loss: 0.1707\n",
            "Iteracja 12200, Loss: 0.1290\n",
            "Iteracja 12300, Loss: 0.1989\n",
            "Iteracja 12400, Loss: 0.1880\n",
            "Iteracja 12500, Loss: 0.1304\n",
            "Iteracja 12600, Loss: 0.1675\n",
            "Iteracja 12700, Loss: 0.1331\n",
            "Iteracja 12800, Loss: 0.1539\n",
            "Iteracja 12900, Loss: 0.1665\n",
            "Iteracja 13000, Loss: 0.1182\n",
            "Iteracja 13100, Loss: 0.1805\n",
            "Iteracja 13200, Loss: 0.2819\n",
            "Iteracja 13300, Loss: 0.1265\n",
            "Iteracja 13400, Loss: 0.1139\n",
            "Iteracja 13500, Loss: 0.1468\n",
            "Iteracja 13600, Loss: 0.1219\n",
            "Iteracja 13700, Loss: 0.1976\n",
            "Iteracja 13800, Loss: 0.1963\n",
            "Iteracja 13900, Loss: 0.2075\n",
            "Iteracja 14000, Loss: 0.0802\n",
            "Iteracja 14100, Loss: 0.1569\n",
            "Iteracja 14200, Loss: 0.1193\n",
            "Iteracja 14300, Loss: 0.1990\n",
            "Iteracja 14400, Loss: 0.2006\n",
            "Iteracja 14500, Loss: 0.0768\n",
            "Iteracja 14600, Loss: 0.1389\n",
            "Iteracja 14700, Loss: 0.2159\n",
            "Iteracja 14800, Loss: 0.2030\n",
            "Iteracja 14900, Loss: 0.2552\n",
            "Iteracja 15000, Loss: 0.1701\n",
            "Iteracja 15100, Loss: 0.2060\n",
            "Iteracja 15200, Loss: 0.1152\n",
            "Iteracja 15300, Loss: 0.1448\n",
            "Iteracja 15400, Loss: 0.1497\n",
            "Iteracja 15500, Loss: 0.2550\n",
            "Iteracja 15600, Loss: 0.1156\n",
            "Iteracja 15700, Loss: 0.1361\n",
            "Iteracja 15800, Loss: 0.0992\n",
            "Iteracja 15900, Loss: 0.1456\n",
            "Iteracja 16000, Loss: 0.1378\n",
            "Iteracja 16100, Loss: 0.1446\n",
            "Iteracja 16200, Loss: 0.1367\n",
            "Iteracja 16300, Loss: 0.1124\n",
            "Iteracja 16400, Loss: 0.1876\n",
            "Iteracja 16500, Loss: 0.1464\n",
            "Iteracja 16600, Loss: 0.1257\n",
            "Iteracja 16700, Loss: 0.1473\n",
            "Iteracja 16800, Loss: 0.1037\n",
            "Iteracja 16900, Loss: 0.1474\n",
            "Iteracja 17000, Loss: 0.0893\n",
            "Iteracja 17100, Loss: 0.1050\n",
            "Iteracja 17200, Loss: 0.1838\n",
            "Iteracja 17300, Loss: 0.1162\n",
            "Iteracja 17400, Loss: 0.1136\n",
            "Iteracja 17500, Loss: 0.1624\n",
            "Iteracja 17600, Loss: 0.1335\n",
            "Iteracja 17700, Loss: 0.1777\n",
            "Iteracja 17800, Loss: 0.1635\n",
            "Iteracja 17900, Loss: 0.1989\n",
            "Iteracja 18000, Loss: 0.1149\n",
            "Iteracja 18100, Loss: 0.1576\n",
            "Iteracja 18200, Loss: 0.1409\n",
            "Iteracja 18300, Loss: 0.1954\n",
            "Iteracja 18400, Loss: 0.2017\n",
            "Iteracja 18500, Loss: 0.2159\n",
            "Iteracja 18600, Loss: 0.0825\n",
            "Iteracja 18700, Loss: 0.0605\n",
            "Iteracja 18800, Loss: 0.1936\n",
            "Iteracja 18900, Loss: 0.0996\n",
            "Iteracja 19000, Loss: 0.1410\n",
            "Iteracja 19100, Loss: 0.1183\n",
            "Iteracja 19200, Loss: 0.1250\n",
            "Iteracja 19300, Loss: 0.0891\n",
            "Iteracja 19400, Loss: 0.1565\n",
            "Iteracja 19500, Loss: 0.1153\n",
            "Iteracja 19600, Loss: 0.1299\n",
            "Iteracja 19700, Loss: 0.1796\n",
            "Iteracja 19800, Loss: 0.1503\n",
            "Iteracja 19900, Loss: 0.0805\n",
            "Iteracja 20000, Loss: 0.1536\n",
            "Iteracja 20100, Loss: 0.1212\n",
            "Iteracja 20200, Loss: 0.1292\n",
            "Iteracja 20300, Loss: 0.0875\n",
            "Iteracja 20400, Loss: 0.0958\n",
            "Iteracja 20500, Loss: 0.1305\n",
            "Iteracja 20600, Loss: 0.1611\n",
            "Iteracja 20700, Loss: 0.1581\n",
            "Iteracja 20800, Loss: 0.0708\n",
            "Iteracja 20900, Loss: 0.1096\n",
            "Iteracja 21000, Loss: 0.1646\n",
            "Iteracja 21100, Loss: 0.1093\n",
            "Iteracja 21200, Loss: 0.1162\n",
            "Iteracja 21300, Loss: 0.1289\n",
            "Iteracja 21400, Loss: 0.1131\n",
            "Iteracja 21500, Loss: 0.1449\n",
            "Iteracja 21600, Loss: 0.0921\n",
            "Iteracja 21700, Loss: 0.1369\n",
            "Iteracja 21800, Loss: 0.1170\n",
            "Iteracja 21900, Loss: 0.0702\n",
            "Iteracja 22000, Loss: 0.0972\n",
            "Iteracja 22100, Loss: 0.0716\n",
            "Iteracja 22200, Loss: 0.1058\n",
            "Iteracja 22300, Loss: 0.0878\n",
            "Iteracja 22400, Loss: 0.1155\n",
            "Iteracja 22500, Loss: 0.1374\n",
            "Iteracja 22600, Loss: 0.1585\n",
            "Iteracja 22700, Loss: 0.0846\n",
            "Iteracja 22800, Loss: 0.1015\n",
            "Iteracja 22900, Loss: 0.1306\n",
            "Iteracja 23000, Loss: 0.0717\n",
            "Iteracja 23100, Loss: 0.1205\n",
            "Iteracja 23200, Loss: 0.0711\n",
            "Iteracja 23300, Loss: 0.0348\n",
            "Iteracja 23400, Loss: 0.1247\n",
            "Iteracja 23500, Loss: 0.1467\n",
            "Iteracja 23600, Loss: 0.1353\n",
            "Iteracja 23700, Loss: 0.0434\n",
            "Iteracja 23800, Loss: 0.1109\n",
            "Iteracja 23900, Loss: 0.1429\n",
            "Iteracja 24000, Loss: 0.1357\n",
            "Iteracja 24100, Loss: 0.1036\n",
            "Iteracja 24200, Loss: 0.0454\n",
            "Iteracja 24300, Loss: 0.1729\n",
            "Iteracja 24400, Loss: 0.0953\n",
            "Iteracja 24500, Loss: 0.0583\n",
            "Iteracja 24600, Loss: 0.0992\n",
            "Iteracja 24700, Loss: 0.1660\n",
            "Iteracja 24800, Loss: 0.1074\n",
            "Iteracja 24900, Loss: 0.0705\n",
            "Iteracja 25000, Loss: 0.0512\n",
            "Iteracja 25100, Loss: 0.1017\n",
            "Iteracja 25200, Loss: 0.1108\n",
            "Iteracja 25300, Loss: 0.0581\n",
            "Iteracja 25400, Loss: 0.0686\n",
            "Iteracja 25500, Loss: 0.0772\n",
            "Iteracja 25600, Loss: 0.0757\n",
            "Iteracja 25700, Loss: 0.0568\n",
            "Iteracja 25800, Loss: 0.0614\n",
            "Iteracja 25900, Loss: 0.0992\n",
            "Iteracja 26000, Loss: 0.0775\n",
            "Iteracja 26100, Loss: 0.0591\n",
            "Iteracja 26200, Loss: 0.0796\n",
            "Iteracja 26300, Loss: 0.1074\n",
            "Iteracja 26400, Loss: 0.1143\n",
            "Iteracja 26500, Loss: 0.1105\n",
            "Iteracja 26600, Loss: 0.0558\n",
            "Iteracja 26700, Loss: 0.1145\n",
            "Iteracja 26800, Loss: 0.0783\n",
            "Iteracja 26900, Loss: 0.1246\n",
            "Iteracja 27000, Loss: 0.1649\n",
            "Iteracja 27100, Loss: 0.1213\n",
            "Iteracja 27200, Loss: 0.0875\n",
            "Iteracja 27300, Loss: 0.0808\n",
            "Iteracja 27400, Loss: 0.0680\n",
            "Iteracja 27500, Loss: 0.0857\n",
            "Iteracja 27600, Loss: 0.1092\n",
            "Iteracja 27700, Loss: 0.1143\n",
            "Iteracja 27800, Loss: 0.0816\n",
            "Iteracja 27900, Loss: 0.0665\n",
            "Iteracja 28000, Loss: 0.0862\n",
            "Iteracja 28100, Loss: 0.0945\n",
            "Iteracja 28200, Loss: 0.1336\n",
            "Iteracja 28300, Loss: 0.1057\n",
            "Iteracja 28400, Loss: 0.1266\n",
            "Iteracja 28500, Loss: 0.1101\n",
            "Iteracja 28600, Loss: 0.1003\n",
            "Iteracja 28700, Loss: 0.0926\n",
            "Iteracja 28800, Loss: 0.1333\n",
            "Iteracja 28900, Loss: 0.1213\n",
            "Iteracja 29000, Loss: 0.0873\n",
            "Iteracja 29100, Loss: 0.1039\n",
            "Iteracja 29200, Loss: 0.0614\n",
            "Iteracja 29300, Loss: 0.1292\n",
            "Iteracja 29400, Loss: 0.1284\n",
            "Iteracja 29500, Loss: 0.0826\n",
            "Iteracja 29600, Loss: 0.0593\n",
            "Iteracja 29700, Loss: 0.0978\n",
            "Iteracja 29800, Loss: 0.0833\n",
            "Iteracja 29900, Loss: 0.0672\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wykres dla relu zawiera ostre krawędzie zamiast bycia falistym i dla ujemnych wartości jest linią prostą."
      ],
      "metadata": {
        "id": "tDA85Dl43CLo"
      }
    }
  ]
}